# This file was generated by ChatGPT

import gensim
import numpy as np


def get_word_embeddings(file_path, model_path):
    # Load pre-trained Word2Vec model
    model = gensim.models.Word2Vec.load(model_path)

    # Initialize variables
    embeddings = {}
    current_block = None

    # Read the text file
    with open(file_path, "r") as file:
        for line in file:
            # Check if this line is the start of a new block
            if line.strip().isdigit():
                # If it is, save the embeddings for the previous block (if any)
                if embeddings and current_block is not None:
                    # Convert the embeddings to a numpy array and save them to the dictionary
                    embeddings[current_block] = np.array(embeddings)

                # Update the current block number
                current_block = int(line.strip())
                # Reset the embeddings list
                embeddings = []

            # Check if this line is the end of a block
            elif not line.strip():
                # If it is, skip it
                pass

            # If this line is part of a block, get its embeddings
            else:
                # Tokenize the line
                tokens = gensim.utils.simple_preprocess(line)
                # Get embeddings for each word
                for token in tokens:
                    try:
                        embedding = model.wv[token]
                        embeddings.append(embedding)
                    except KeyError:
                        # If the word is not in the vocabulary, skip it
                        pass

    # Save the embeddings for the last block (if any)
    if embeddings and current_block is not None:
        # Convert the embeddings to a numpy array and save them to the dictionary
        embeddings[current_block] = np.array(embeddings)

    return list(embeddings.values())
